# 🚀 vLLM vs Transformers 性能对比工具

一个全面、客观的深度学习推理框架性能对比工具，帮助开发者在 vLLM 和 Transformers 之间做出明智的技术选择。

## 📋 项目简介

本工具通过实际测试数据，从多个维度对比 vLLM 和 Transformers 两个主流推理框架的性能表现，为生产环境的技术选型提供科学依据。

### 🎯 主要功能

- **模型加载性能测试** - 对比加载时间和内存占用
- **单次推理性能测试** - 测试延迟和Token生成速度
- **批处理性能测试** - 评估不同批处理大小下的吞吐量
- **真实场景模拟** - 聊天机器人、内容生成、代码助手等场景
- **详细性能报告** - 生成完整的JSON格式测试报告
- **直观对比表格** - 清晰展示各项性能指标差异

## 🛠️ 技术特点

- ✅ **完全客观** - 相同硬件、相同模型、相同参数配置
- ✅ **全面覆盖** - 从加载到推理，从单次到批处理
- ✅ **真实场景** - 模拟实际生产环境使用情况
- ✅ **详细记录** - 完整的实验条件和硬件信息
- ✅ **易于理解** - 直观的表格和性能总结

## 📦 依赖要求

```bash
# 必需依赖
pip install torch transformers vllm psutil

# 可选依赖（用于GPU支持）
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
```

### 系统要求

- Python 3.8+
- CUDA 11.8+ (推荐用于GPU加速)
- 8GB+ 内存
- 2GB+ GPU显存 (使用GPU时)

## 🚀 快速开始

### 基本使用

```bash
python vllmvstransformers.py
```

### 运行流程

1. **环境检测** - 自动检测硬件配置和软件版本
2. **模型加载测试** - 分别测试两个框架的加载性能
3. **单次推理测试** - 使用相同提示词测试推理性能
4. **批处理测试** - 测试1、5、10、20个请求的批处理性能
5. **场景测试** - 模拟聊天、内容生成、代码助手场景
6. **生成报告** - 输出详细的性能对比报告

## 🧪 测试模式详解

### 测试模型
- **模型名称**: `facebook/opt-125m`
- **参数量**: 125,000,000 (1.25亿参数)
- **模型类型**: 因果语言模型 (Causal LM)
- **开发者**: Meta (Facebook)
- **模型架构**: OPT (Open Pre-trained Transformer)

### 测试配置
- **推理精度**: FP16 (半精度浮点)
- **温度参数**: 0.8 (中等随机性)
- **Top-p参数**: 0.95 (核采样)
- **最大Token数**: 50 (标准测试), 80 (场景测试)
- **GPU内存利用率**: 80%
- **批处理大小**: 1, 5, 10, 20 (递进测试)

### 测试流程
1. **环境检测阶段**
   - 自动检测硬件配置 (CPU、GPU、内存)
   - 记录软件版本 (Python、PyTorch、CUDA、库版本)
   - 输出完整的实验条件报告

2. **模型加载测试**
   - 分别测试两个框架的冷启动时间
   - 监控GPU和系统内存占用
   - 每次测试前清理缓存确保公平性

3. **单次推理测试**
   - 使用相同提示词: "Hello, my name is"
   - 测试首次推理的延迟和Token生成速度
   - 记录输出质量和Token数量

4. **批处理性能测试**
   - 测试1、5、10、20个请求的并发处理
   - Transformers: 串行处理 (循环调用)
   - vLLM: 真正的并行批处理
   - 计算吞吐量和平均延迟

5. **真实场景模拟**
   - **聊天机器人**: 5轮连续对话测试
   - **内容生成**: 批量创作任务
   - **代码助手**: 编程问题解答
   - 评估用户体验和响应时间

### 测试公平性保证
- ✅ **相同硬件环境**: 同一台机器、同一GPU
- ✅ **相同模型配置**: 同一模型、同一参数设置
- ✅ **相同测试数据**: 完全一致的输入提示词
- ✅ **缓存清理**: 每次测试前清理GPU缓存
- ✅ **多次测量**: 避免偶然因素影响结果
- ✅ **客观记录**: 自动化测试，避免人为误差

## 📊 测试结果解读

### 性能指标说明

| 指标 | 说明 | 优化目标 |
|------|------|----------|
| 加载时间 | 模型初始化所需时间 | 越短越好 |
| GPU内存 | 模型加载后GPU内存占用 | 越少越好 |
| 推理时间 | 单次推理所需时间 | 越短越好 |
| Token生成速度 | 每秒生成Token数量 | 越高越好 |
| 吞吐量 | 每秒处理请求数量 | 越高越好 |

### 输出文件说明

- `intuitive_comparison_YYYYMMDD_HHMMSS.json` - 完整测试数据
- `comparison_summary_YYYYMMDD_HHMMSS.json` - 简化摘要报告

## 🔧 自定义配置

你可以修改脚本中的以下参数：

```python
# 模型配置
self.model_name = "facebook/opt-125m"  # 更换测试模型

# 推理参数
temperature=0.8      # 随机性控制
max_tokens=50       # 最大生成Token数
top_p=0.95          # 核采样参数

# 批处理测试
batch_sizes = [1, 5, 10, 20]  # 批处理大小
```

## 📈 典型测试结果

### 性能对比示例

```
📊 关键性能指标总结
================================================================================
指标                 Transformers    vLLM           差异            winner    
--------------------------------------------------------------------------------
模型加载时间(s)        3.2            5.1            1.6x slower     🤗 Trans  
单次推理时间(s)        0.45           0.28           1.6x slower     🚀 vLLM   
批处理吞吐量(req/s)     12.5           45.8           3.7x faster     🚀 vLLM   
GPU内存使用(GB)        0.8            1.2            1.5x more       🤗 Trans  
```

### 使用场景推荐

**推荐使用 vLLM 的场景：**
- 生产环境 API 服务
- 需要高吞吐量的应用
- 实时对话应用
- 批处理推理任务

**推荐使用 Transformers 的场景：**
- 快速原型开发
- 学习和研究
- 需要丰富预处理功能
- 对部署复杂度敏感的项目

## 🤝 贡献指南

欢迎提交问题和改进建议！

1. Fork 本项目
2. 创建功能分支 (`git checkout -b feature/amazing-feature`)
3. 提交更改 (`git commit -m 'Add amazing feature'`)
4. 推送到分支 (`git push origin feature/amazing-feature`)
5. 创建 Pull Request

## 📄 许可证

本项目使用 MIT 许可证 - 详见 [LICENSE](LICENSE) 文件

## 🙏 致谢

- [vLLM](https://github.com/vllm-project/vllm) - 高性能推理引擎
- [Transformers](https://github.com/huggingface/transformers) - 强大的NLP库
- [Facebook OPT](https://github.com/facebookresearch/metaseq) - 测试模型

## 📞 联系方式

如有问题或建议，请创建 Issue 或联系开发者。

---

**注意：** 测试结果可能因硬件配置、模型大小、系统负载等因素而有所不同。建议在自己的环境中运行测试以获得最准确的性能数据。 
