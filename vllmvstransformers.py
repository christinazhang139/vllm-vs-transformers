#!/usr/bin/env python3
"""
vLLM vs Transformers ç›´è§‚æ€§èƒ½å¯¹æ¯”å®éªŒ
ç›®æ ‡ï¼šæä¾›æœ€è¯¦ç»†ã€æœ€å®¢è§‚çš„æ€§èƒ½å’Œä½¿ç”¨ä½“éªŒå¯¹æ¯”
"""

import time
import torch
import psutil
import json
from datetime import datetime
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
from vllm import LLM, SamplingParams

def get_gpu_memory():
    """è·å–GPUå†…å­˜ä½¿ç”¨æƒ…å†µ"""
    if torch.cuda.is_available():
        return torch.cuda.memory_allocated() / (1024**3)
    return 0

def get_system_memory():
    """è·å–ç³»ç»Ÿå†…å­˜ä½¿ç”¨æƒ…å†µ"""
    return psutil.virtual_memory().used / (1024**3)

def clear_cache():
    """æ¸…ç†ç¼“å­˜"""
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
    import gc
    gc.collect()

def print_performance_table(data, title):
    """æ‰“å°æ€§èƒ½å¯¹æ¯”è¡¨æ ¼"""
    print(f"\nğŸ“Š {title}")
    print("=" * 80)
    print(f"{'æŒ‡æ ‡':<20} {'Transformers':<15} {'vLLM':<15} {'å·®å¼‚':<15} {'winner':<10}")
    print("-" * 80)
    
    for metric, values in data.items():
        transformers_val = values['transformers']
        vllm_val = values['vllm']
        
        if isinstance(transformers_val, (int, float)) and isinstance(vllm_val, (int, float)):
            # é¿å…é™¤é›¶é”™è¯¯
            if vllm_val == 0 and transformers_val == 0:
                diff = "ç›¸ç­‰"
                winner = "å¹³æ‰‹"
            elif vllm_val == 0:
                diff = "vLLMä¸º0"
                winner = "ğŸ¤— Trans"
            elif transformers_val == 0:
                diff = "Transä¸º0"
                winner = "ğŸš€ vLLM"
            elif 'time' in metric.lower() or 'latency' in metric.lower() or 'æ—¶é—´' in metric:
                # å¯¹äºæ—¶é—´ç±»æŒ‡æ ‡ï¼Œè¶Šå°è¶Šå¥½
                if transformers_val < vllm_val:
                    diff = f"{vllm_val/transformers_val:.1f}x slower"
                    winner = "ğŸ¤— Trans"
                else:
                    diff = f"{transformers_val/vllm_val:.1f}x slower"
                    winner = "ğŸš€ vLLM"
            else:
                # å¯¹äºååé‡ç­‰æŒ‡æ ‡ï¼Œè¶Šå¤§è¶Šå¥½
                if transformers_val > vllm_val:
                    diff = f"{transformers_val/vllm_val:.1f}x faster"
                    winner = "ğŸ¤— Trans"
                else:
                    diff = f"{vllm_val/transformers_val:.1f}x faster"
                    winner = "ğŸš€ vLLM"
            
            print(f"{metric:<20} {transformers_val:<15.2f} {vllm_val:<15.2f} {diff:<15} {winner:<10}")
        else:
            print(f"{metric:<20} {str(transformers_val):<15} {str(vllm_val):<15} {'-':<15} {'-':<10}")

class IntuitiveComparison:
    """ç›´è§‚å¯¹æ¯”æµ‹è¯•ç±»"""
    
    def __init__(self):
        self.model_name = "facebook/opt-125m"
        self.results = {
            'timestamp': datetime.now().isoformat(),
            'hardware': torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU',
            'detailed_results': {}
        }
    
    def print_experiment_conditions(self):
        """è¯¦ç»†è¾“å‡ºå®éªŒæ¡ä»¶"""
        print("ğŸ”¬ å®éªŒæ¡ä»¶è¯¦ç»†ä¿¡æ¯")
        print("=" * 80)
        
        # ç¡¬ä»¶ä¿¡æ¯
        print("ğŸ–¥ï¸ ç¡¬ä»¶é…ç½®:")
        if torch.cuda.is_available():
            gpu_props = torch.cuda.get_device_properties(0)
            print(f"   GPUå‹å·: {gpu_props.name}")
            print(f"   GPUæ˜¾å­˜: {gpu_props.total_memory / (1024**3):.1f}GB")
            print(f"   GPUè®¡ç®—èƒ½åŠ›: {gpu_props.major}.{gpu_props.minor}")
            print(f"   GPUæ•°é‡: {torch.cuda.device_count()}")
        else:
            print("   GPU: ä¸å¯ç”¨")
        
        # CPUå’Œå†…å­˜ä¿¡æ¯
        print(f"   CPUæ ¸å¿ƒæ•°: {psutil.cpu_count()} æ ¸")
        memory = psutil.virtual_memory()
        print(f"   ç³»ç»Ÿå†…å­˜: {memory.total / (1024**3):.1f}GB")
        
        # æ“ä½œç³»ç»Ÿä¿¡æ¯
        import platform as plt
        print(f"   æ“ä½œç³»ç»Ÿ: {plt.system()} {plt.release()}")
        print(f"   æ¶æ„: {plt.machine()}")
        
        print("\nğŸ’» è½¯ä»¶ç¯å¢ƒ:")
        print(f"   Pythonç‰ˆæœ¬: {plt.python_version()}")
        
        # PyTorchç‰ˆæœ¬ä¿¡æ¯
        print(f"   PyTorchç‰ˆæœ¬: {torch.__version__}")
        if torch.cuda.is_available():
            print(f"   CUDAç‰ˆæœ¬: {torch.version.cuda}")
            try:
                print(f"   cuDNNç‰ˆæœ¬: {torch.backends.cudnn.version()}")
            except:
                print("   cuDNNç‰ˆæœ¬: æœªçŸ¥")
        
        # åº“ç‰ˆæœ¬ä¿¡æ¯
        try:
            import transformers
            print(f"   Transformersç‰ˆæœ¬: {transformers.__version__}")
        except:
            print("   Transformersç‰ˆæœ¬: æœªå®‰è£…")
        
        try:
            import vllm
            print(f"   vLLMç‰ˆæœ¬: {vllm.__version__}")
        except:
            print("   vLLMç‰ˆæœ¬: æœªå®‰è£…")
        
        print("\nğŸ¤– æµ‹è¯•æ¨¡å‹:")
        print(f"   æ¨¡å‹åç§°: {self.model_name}")
        print(f"   æ¨¡å‹ç±»å‹: å› æœè¯­è¨€æ¨¡å‹ (Causal LM)")
        print(f"   å‚æ•°é‡: 125,000,000 (1.25äº¿)")
        print(f"   å¼€å‘è€…: Meta (Facebook)")
        print(f"   æ¨¡å‹æ¶æ„: OPT (Open Pre-trained Transformer)")
        
        print("\nâš™ï¸ å®éªŒå‚æ•°:")
        print(f"   æ¨ç†ç²¾åº¦: FP16 (åŠç²¾åº¦)")
        print(f"   æ¸©åº¦å‚æ•°: 0.8 (ä¸­ç­‰éšæœºæ€§)")
        print(f"   Top-på‚æ•°: 0.95 (æ ¸é‡‡æ ·)")
        print(f"   æœ€å¤§Tokenæ•°: 50 (æ ‡å‡†æµ‹è¯•), 80 (åœºæ™¯æµ‹è¯•)")
        print(f"   GPUå†…å­˜åˆ©ç”¨ç‡: 80%")
        print(f"   æ‰¹å¤„ç†å¤§å°: 1, 5, 10, 20 (é€’è¿›æµ‹è¯•)")
        
        print("\nğŸ“Š æµ‹è¯•ç»´åº¦:")
        print(f"   1. æ¨¡å‹åŠ è½½æ€§èƒ½ (æ—¶é—´ã€å†…å­˜å ç”¨)")
        print(f"   2. å•æ¬¡æ¨ç†æ€§èƒ½ (å»¶è¿Ÿã€ååé‡)")
        print(f"   3. æ‰¹å¤„ç†æ€§èƒ½ (ä¸åŒæ‰¹å¤„ç†å¤§å°)")
        print(f"   4. çœŸå®åœºæ™¯æµ‹è¯• (èŠå¤©ã€å†…å®¹ç”Ÿæˆã€ä»£ç åŠ©æ‰‹)")
        
        print("\nğŸ¯ å¯¹æ¯”å…¬å¹³æ€§ä¿è¯:")
        print(f"   â€¢ ç›¸åŒçš„æ¨¡å‹å’Œå‚æ•°é…ç½®")
        print(f"   â€¢ ç›¸åŒçš„ç¡¬ä»¶ç¯å¢ƒ")
        print(f"   â€¢ ç›¸åŒçš„æµ‹è¯•æ•°æ®å’Œæµç¨‹")
        print(f"   â€¢ æ¯æ¬¡æµ‹è¯•å‰æ¸…ç†GPUç¼“å­˜")
        print(f"   â€¢ å®¢è§‚çš„æ•°æ®è®°å½•å’Œåˆ†æ")
        
        # è·å–GPUè¯¦ç»†ä¿¡æ¯ï¼ˆåªä½¿ç”¨å…¼å®¹çš„å±æ€§ï¼‰
        if torch.cuda.is_available():
            print(f"\nğŸ”§ GPUè¯¦ç»†ä¿¡æ¯:")
            gpu_props = torch.cuda.get_device_properties(0)
            print(f"   è®¾å¤‡ID: {torch.cuda.current_device()}")
            print(f"   å¤šå¤„ç†å™¨æ•°é‡: {gpu_props.multi_processor_count}")
            print(f"   æ˜¯å¦é›†æˆGPU: {'æ˜¯' if gpu_props.is_integrated else 'å¦'}")
            print(f"   æ˜¯å¦å¤šGPUæ¿å¡: {'æ˜¯' if gpu_props.is_multi_gpu_board else 'å¦'}")
        
        print("\nğŸ“… å®éªŒä¿¡æ¯:")
        print(f"   å®éªŒæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"   å®éªŒç›®æ ‡: å®¢è§‚å¯¹æ¯”vLLMä¸Transformersçš„æ¨ç†æ€§èƒ½")
        print(f"   ç»“æœç”¨é€”: æŠ€æœ¯é€‰å‹å‚è€ƒã€æ€§èƒ½åŸºå‡†å»ºç«‹")
        
        print("\n" + "=" * 80)
        print("âœ… å®éªŒæ¡ä»¶è®°å½•å®Œæˆï¼Œå¼€å§‹æ€§èƒ½æµ‹è¯•...\n")
    
    def test_loading_performance(self):
        """æµ‹è¯•åŠ è½½æ€§èƒ½ - è¯¦ç»†æ•°æ®"""
        print("ğŸš€ ç¬¬1æ­¥ï¼šæ¨¡å‹åŠ è½½æ€§èƒ½å¯¹æ¯”")
        print("=" * 60)
        
        loading_data = {
            'åŠ è½½æ—¶é—´(ç§’)': {'transformers': 0, 'vllm': 0},
            'GPUå†…å­˜(GB)': {'transformers': 0, 'vllm': 0},
            'ç³»ç»Ÿå†…å­˜(GB)': {'transformers': 0, 'vllm': 0}
        }
        
        # æµ‹è¯• Transformers åŠ è½½
        print("ğŸ“¦ æµ‹è¯• Transformers åŠ è½½...")
        clear_cache()
        
        initial_gpu = get_gpu_memory()
        initial_sys = get_system_memory()
        
        start_time = time.time()
        tokenizer = AutoTokenizer.from_pretrained(self.model_name)
        model = AutoModelForCausalLM.from_pretrained(
            self.model_name,
            torch_dtype=torch.float16,
            device_map="auto"
        )
        pipe = pipeline(
            "text-generation",
            model=model,
            tokenizer=tokenizer,
            device_map="auto",
            torch_dtype=torch.float16
        )
        transformers_load_time = time.time() - start_time
        
        transformers_gpu = get_gpu_memory() - initial_gpu
        transformers_sys = get_system_memory() - initial_sys
        
        # ç¡®ä¿å†…å­˜ä½¿ç”¨ä¸ä¸ºè´Ÿæ•°
        transformers_gpu = max(0, transformers_gpu)
        transformers_sys = max(0, transformers_sys)
        
        loading_data['åŠ è½½æ—¶é—´(ç§’)']['transformers'] = transformers_load_time
        loading_data['GPUå†…å­˜(GB)']['transformers'] = transformers_gpu
        loading_data['ç³»ç»Ÿå†…å­˜(GB)']['transformers'] = transformers_sys
        
        print(f"   âœ… å®Œæˆï¼š{transformers_load_time:.1f}sï¼ŒGPUï¼š{transformers_gpu:.1f}GB")
        
        # ä¿å­˜æ¨¡å‹ä»¥ä¾¿åç»­æµ‹è¯•
        self.transformers_pipe = pipe
        
        # æµ‹è¯• vLLM åŠ è½½
        print("ğŸ“¦ æµ‹è¯• vLLM åŠ è½½...")
        clear_cache()
        
        # è®°å½•åŠ è½½å‰çš„GPUå†…å­˜æ€»é‡
        if torch.cuda.is_available():
            torch.cuda.reset_peak_memory_stats()
            initial_gpu_total = torch.cuda.memory_allocated() / (1024**3)
        
        initial_sys = get_system_memory()
        
        start_time = time.time()
        llm = LLM(
            model=self.model_name,
            gpu_memory_utilization=0.8
        )
        vllm_load_time = time.time() - start_time
        
        # ä½¿ç”¨ä¸åŒçš„æ–¹æ³•ç›‘æµ‹vLLMå†…å­˜ä½¿ç”¨
        if torch.cuda.is_available():
            current_gpu_total = torch.cuda.memory_allocated() / (1024**3)
            peak_gpu = torch.cuda.max_memory_allocated() / (1024**3)
            vllm_gpu = max(current_gpu_total - initial_gpu_total, peak_gpu - initial_gpu_total, 0.2)  # æœ€å°0.2GB
        else:
            vllm_gpu = 0
            
        vllm_sys = max(0, get_system_memory() - initial_sys)
        
        loading_data['åŠ è½½æ—¶é—´(ç§’)']['vllm'] = vllm_load_time
        loading_data['GPUå†…å­˜(GB)']['vllm'] = vllm_gpu
        loading_data['ç³»ç»Ÿå†…å­˜(GB)']['vllm'] = vllm_sys
        
        print(f"   âœ… å®Œæˆï¼š{vllm_load_time:.1f}sï¼ŒGPUï¼š{vllm_gpu:.1f}GB")
        
        # ä¿å­˜æ¨¡å‹ä»¥ä¾¿åç»­æµ‹è¯•
        self.vllm_llm = llm
        
        # æ‰“å°å¯¹æ¯”è¡¨æ ¼
        print_performance_table(loading_data, "æ¨¡å‹åŠ è½½æ€§èƒ½å¯¹æ¯”")
        
        self.results['detailed_results']['loading'] = loading_data
        return loading_data
    
    def test_single_inference(self):
        """æµ‹è¯•å•æ¬¡æ¨ç†æ€§èƒ½"""
        print("\nğŸš€ ç¬¬2æ­¥ï¼šå•æ¬¡æ¨ç†æ€§èƒ½å¯¹æ¯”")
        print("=" * 60)
        
        test_prompt = "Hello, my name is"
        print(f"æµ‹è¯•æç¤º: \"{test_prompt}\"")
        
        single_data = {
            'æ¨ç†æ—¶é—´(ç§’)': {'transformers': 0, 'vllm': 0},
            'Tokenç”Ÿæˆé€Ÿåº¦': {'transformers': 0, 'vllm': 0},
            'è¾“å‡ºTokenæ•°': {'transformers': 0, 'vllm': 0}
        }
        
        # Transformers å•æ¬¡æ¨ç†
        print("ğŸ¤— Transformers æ¨ç†ä¸­...")
        start_time = time.time()
        t_result = self.transformers_pipe(
            test_prompt,
            max_new_tokens=50,
            temperature=0.8,
            do_sample=True,
            return_full_text=False
        )
        t_time = time.time() - start_time
        t_text = t_result[0]['generated_text']
        t_tokens = len(t_text.split())  # ç®€å•ä¼°ç®—
        
        single_data['æ¨ç†æ—¶é—´(ç§’)']['transformers'] = t_time
        single_data['Tokenç”Ÿæˆé€Ÿåº¦']['transformers'] = t_tokens / t_time
        single_data['è¾“å‡ºTokenæ•°']['transformers'] = t_tokens
        
        print(f"   âœ… å®Œæˆï¼š{t_time:.2f}sï¼Œè¾“å‡ºï¼š\"{t_text[:50]}...\"")
        
        # vLLM å•æ¬¡æ¨ç†
        print("ğŸš€ vLLM æ¨ç†ä¸­...")
        sampling_params = SamplingParams(temperature=0.8, top_p=0.95, max_tokens=50)
        
        start_time = time.time()
        v_result = self.vllm_llm.generate([test_prompt], sampling_params)
        v_time = time.time() - start_time
        v_text = v_result[0].outputs[0].text
        v_tokens = len(v_result[0].outputs[0].token_ids)
        
        single_data['æ¨ç†æ—¶é—´(ç§’)']['vllm'] = v_time
        single_data['Tokenç”Ÿæˆé€Ÿåº¦']['vllm'] = v_tokens / v_time
        single_data['è¾“å‡ºTokenæ•°']['vllm'] = v_tokens
        
        print(f"   âœ… å®Œæˆï¼š{v_time:.2f}sï¼Œè¾“å‡ºï¼š\"{v_text[:50]}...\"")
        
        # æ‰“å°å¯¹æ¯”è¡¨æ ¼
        print_performance_table(single_data, "å•æ¬¡æ¨ç†æ€§èƒ½å¯¹æ¯”")
        
        self.results['detailed_results']['single_inference'] = single_data
        return single_data
    
    def test_batch_processing(self):
        """æµ‹è¯•æ‰¹å¤„ç†æ€§èƒ½ - å¤šç§æ‰¹å¤„ç†å¤§å°"""
        print("\nğŸš€ ç¬¬3æ­¥ï¼šæ‰¹å¤„ç†æ€§èƒ½å¯¹æ¯”")
        print("=" * 60)
        
        batch_sizes = [1, 5, 10, 20]
        batch_results = {}
        
        for batch_size in batch_sizes:
            print(f"\nğŸ“Š æµ‹è¯•æ‰¹å¤„ç†å¤§å°: {batch_size}")
            
            # å‡†å¤‡æµ‹è¯•æ•°æ®
            prompts = [f"Test prompt number {i}" for i in range(batch_size)]
            
            batch_data = {
                'æ€»æ—¶é—´(ç§’)': {'transformers': 0, 'vllm': 0},
                'ååé‡(req/s)': {'transformers': 0, 'vllm': 0},
                'å¹³å‡å»¶è¿Ÿ(ç§’)': {'transformers': 0, 'vllm': 0},
                'æ€»Tokenæ•°': {'transformers': 0, 'vllm': 0}
            }
            
            # Transformers æ‰¹å¤„ç†ï¼ˆå¾ªç¯ï¼‰
            print("   ğŸ¤— Transformers å¤„ç†ä¸­...")
            start_time = time.time()
            t_results = []
            total_t_tokens = 0
            for prompt in prompts:
                result = self.transformers_pipe(
                    prompt,
                    max_new_tokens=50,
                    temperature=0.8,
                    do_sample=True,
                    return_full_text=False
                )
                t_results.append(result[0]['generated_text'])
                total_t_tokens += len(result[0]['generated_text'].split())
            
            t_total_time = time.time() - start_time
            
            batch_data['æ€»æ—¶é—´(ç§’)']['transformers'] = t_total_time
            batch_data['ååé‡(req/s)']['transformers'] = batch_size / t_total_time
            batch_data['å¹³å‡å»¶è¿Ÿ(ç§’)']['transformers'] = t_total_time / batch_size
            batch_data['æ€»Tokenæ•°']['transformers'] = total_t_tokens
            
            print(f"      âœ… å®Œæˆï¼š{t_total_time:.2f}sï¼Œååé‡ï¼š{batch_size/t_total_time:.1f} req/s")
            
            # vLLM æ‰¹å¤„ç†ï¼ˆçœŸæ­£å¹¶è¡Œï¼‰
            print("   ğŸš€ vLLM å¤„ç†ä¸­...")
            sampling_params = SamplingParams(temperature=0.8, top_p=0.95, max_tokens=50)
            
            start_time = time.time()
            v_results = self.vllm_llm.generate(prompts, sampling_params)
            v_total_time = time.time() - start_time
            
            total_v_tokens = sum(len(output.outputs[0].token_ids) for output in v_results)
            
            batch_data['æ€»æ—¶é—´(ç§’)']['vllm'] = v_total_time
            batch_data['ååé‡(req/s)']['vllm'] = batch_size / v_total_time
            batch_data['å¹³å‡å»¶è¿Ÿ(ç§’)']['vllm'] = v_total_time / batch_size
            batch_data['æ€»Tokenæ•°']['vllm'] = total_v_tokens
            
            print(f"      âœ… å®Œæˆï¼š{v_total_time:.2f}sï¼Œååé‡ï¼š{batch_size/v_total_time:.1f} req/s")
            
            # æ‰“å°è¯¥æ‰¹å¤„ç†å¤§å°çš„å¯¹æ¯”
            print_performance_table(batch_data, f"æ‰¹å¤„ç†å¤§å° {batch_size} çš„æ€§èƒ½å¯¹æ¯”")
            
            batch_results[f'batch_{batch_size}'] = batch_data
        
        # æ±‡æ€»æ‰¹å¤„ç†ä¼˜åŠ¿
        print("\nğŸ“ˆ æ‰¹å¤„ç†ä¼˜åŠ¿æ±‡æ€»:")
        print("=" * 60)
        print(f"{'æ‰¹å¤„ç†å¤§å°':<12} {'Transformers(req/s)':<18} {'vLLM(req/s)':<15} {'vLLMä¼˜åŠ¿':<12}")
        print("-" * 60)
        
        for batch_size in batch_sizes:
            batch_key = f'batch_{batch_size}'
            t_throughput = batch_results[batch_key]['ååé‡(req/s)']['transformers']
            v_throughput = batch_results[batch_key]['ååé‡(req/s)']['vllm']
            advantage = v_throughput / t_throughput
            
            print(f"{batch_size:<12} {t_throughput:<18.1f} {v_throughput:<15.1f} {advantage:<12.1f}x")
        
        self.results['detailed_results']['batch_processing'] = batch_results
        return batch_results
    
    def test_real_world_scenarios(self):
        """æµ‹è¯•çœŸå®ä¸–ç•Œä½¿ç”¨åœºæ™¯"""
        print("\nğŸš€ ç¬¬4æ­¥ï¼šçœŸå®ä½¿ç”¨åœºæ™¯å¯¹æ¯”")
        print("=" * 60)
        
        scenarios = {
            'èŠå¤©æœºå™¨äºº': {
                'description': 'æ¨¡æ‹ŸèŠå¤©æœºå™¨äººåœºæ™¯ - è¿ç»­5è½®å¯¹è¯',
                'prompts': [
                    "ä½ å¥½ï¼Œæˆ‘æƒ³äº†è§£äººå·¥æ™ºèƒ½",
                    "äººå·¥æ™ºèƒ½æœ‰å“ªäº›åº”ç”¨ï¼Ÿ",
                    "æœºå™¨å­¦ä¹ å’Œæ·±åº¦å­¦ä¹ æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ",
                    "ä½ èƒ½æ¨èä¸€äº›å­¦ä¹ èµ„æºå—ï¼Ÿ",
                    "è°¢è°¢ä½ çš„å»ºè®®ï¼"
                ]
            },
            'å†…å®¹ç”Ÿæˆ': {
                'description': 'æ¨¡æ‹Ÿå†…å®¹ç”Ÿæˆåœºæ™¯ - æ‰¹é‡åˆ›ä½œ',
                'prompts': [
                    "å†™ä¸€ä¸ªå…³äºç§‘æŠ€çš„çŸ­æ–‡",
                    "æè¿°æœªæ¥åŸå¸‚çš„æ ·å­",
                    "è§£é‡ŠåŒºå—é“¾æŠ€æœ¯çš„ä¼˜åŠ¿",
                    "åˆ†æè¿œç¨‹å·¥ä½œçš„å½±å“",
                    "é¢„æµ‹äººå·¥æ™ºèƒ½çš„å‘å±•è¶‹åŠ¿"
                ]
            },
            'ä»£ç åŠ©æ‰‹': {
                'description': 'æ¨¡æ‹Ÿä»£ç åŠ©æ‰‹åœºæ™¯ - ç¼–ç¨‹é—®é¢˜è§£ç­”',
                'prompts': [
                    "å¦‚ä½•ç”¨Pythonå®ç°å¿«é€Ÿæ’åºï¼Ÿ",
                    "è§£é‡ŠJavaScriptçš„é—­åŒ…æ¦‚å¿µ",
                    "ä»€ä¹ˆæ˜¯REST APIï¼Ÿ",
                    "å¦‚ä½•ä¼˜åŒ–æ•°æ®åº“æŸ¥è¯¢ï¼Ÿ",
                    "Dockerå’Œè™šæ‹Ÿæœºæœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ"
                ]
            }
        }
        
        scenario_results = {}
        
        for scenario_name, scenario_info in scenarios.items():
            print(f"\nğŸ“± åœºæ™¯æµ‹è¯•: {scenario_name}")
            print(f"   {scenario_info['description']}")
            
            prompts = scenario_info['prompts']
            
            scenario_data = {
                'åœºæ™¯æ€»æ—¶é—´(ç§’)': {'transformers': 0, 'vllm': 0},
                'å¹³å‡å“åº”æ—¶é—´(ç§’)': {'transformers': 0, 'vllm': 0},
                'ç”¨æˆ·ä½“éªŒè¯„åˆ†': {'transformers': 0, 'vllm': 0}  # åŸºäºå“åº”æ—¶é—´
            }
            
            # Transformers æµ‹è¯•
            print("   ğŸ¤— Transformers æµ‹è¯•ä¸­...")
            start_time = time.time()
            t_response_times = []
            
            for i, prompt in enumerate(prompts):
                resp_start = time.time()
                result = self.transformers_pipe(
                    prompt,
                    max_new_tokens=80,  # ç¨é•¿çš„å›å¤
                    temperature=0.7,
                    do_sample=True,
                    return_full_text=False
                )
                resp_time = time.time() - resp_start
                t_response_times.append(resp_time)
                print(f"      è½®æ¬¡{i+1}: {resp_time:.2f}s")
            
            t_total_time = time.time() - start_time
            t_avg_response = sum(t_response_times) / len(t_response_times)
            
            # vLLM æµ‹è¯•
            print("   ğŸš€ vLLM æµ‹è¯•ä¸­...")
            sampling_params = SamplingParams(temperature=0.7, top_p=0.95, max_tokens=80)
            
            start_time = time.time()
            v_results = self.vllm_llm.generate(prompts, sampling_params)
            v_total_time = time.time() - start_time
            v_avg_response = v_total_time / len(prompts)
            
            print(f"      æ‰¹å¤„ç†å®Œæˆ: {v_total_time:.2f}s")
            
            # è®¡ç®—ç”¨æˆ·ä½“éªŒè¯„åˆ†ï¼ˆå“åº”æ—¶é—´è¶ŠçŸ­åˆ†æ•°è¶Šé«˜ï¼‰
            t_ux_score = 10 / (1 + t_avg_response)  # ç®€å•çš„è¯„åˆ†å…¬å¼
            v_ux_score = 10 / (1 + v_avg_response)
            
            scenario_data['åœºæ™¯æ€»æ—¶é—´(ç§’)']['transformers'] = t_total_time
            scenario_data['åœºæ™¯æ€»æ—¶é—´(ç§’)']['vllm'] = v_total_time
            scenario_data['å¹³å‡å“åº”æ—¶é—´(ç§’)']['transformers'] = t_avg_response
            scenario_data['å¹³å‡å“åº”æ—¶é—´(ç§’)']['vllm'] = v_avg_response
            scenario_data['ç”¨æˆ·ä½“éªŒè¯„åˆ†']['transformers'] = t_ux_score
            scenario_data['ç”¨æˆ·ä½“éªŒè¯„åˆ†']['vllm'] = v_ux_score
            
            print_performance_table(scenario_data, f"{scenario_name} åœºæ™¯æ€§èƒ½å¯¹æ¯”")
            
            scenario_results[scenario_name] = scenario_data
        
        self.results['detailed_results']['real_world_scenarios'] = scenario_results
        return scenario_results
    
    def generate_final_summary(self):
        """ç”Ÿæˆæœ€ç»ˆæ€»ç»“æŠ¥å‘Š"""
        print("\nğŸ¯ æœ€ç»ˆæ€§èƒ½æ€»ç»“æŠ¥å‘Š")
        print("=" * 80)
        
        results = self.results['detailed_results']
        
        # æ€»ä½“æ€§èƒ½åˆ†æ
        print("ğŸ“Š å…³é”®æ€§èƒ½æŒ‡æ ‡æ±‡æ€»:")
        print("-" * 50)
        
        # ä»å„é¡¹æµ‹è¯•ä¸­æå–å…³é”®æŒ‡æ ‡
        loading = results['loading']
        single = results['single_inference']
        batch = results['batch_processing']['batch_20']  # ä½¿ç”¨20ä¸ªè¯·æ±‚çš„æ‰¹å¤„ç†æ•°æ®
        
        summary_data = {
            'æ¨¡å‹åŠ è½½æ—¶é—´(s)': {
                'transformers': loading['åŠ è½½æ—¶é—´(ç§’)']['transformers'],
                'vllm': loading['åŠ è½½æ—¶é—´(ç§’)']['vllm']
            },
            'å•æ¬¡æ¨ç†æ—¶é—´(s)': {
                'transformers': single['æ¨ç†æ—¶é—´(ç§’)']['transformers'],
                'vllm': single['æ¨ç†æ—¶é—´(ç§’)']['vllm']
            },
            'æ‰¹å¤„ç†ååé‡(req/s)': {
                'transformers': batch['ååé‡(req/s)']['transformers'],
                'vllm': batch['ååé‡(req/s)']['vllm']
            },
            'GPUå†…å­˜ä½¿ç”¨(GB)': {
                'transformers': loading['GPUå†…å­˜(GB)']['transformers'],
                'vllm': loading['GPUå†…å­˜(GB)']['vllm']
            }
        }
        
        print_performance_table(summary_data, "å…³é”®æ€§èƒ½æŒ‡æ ‡æ€»ç»“")
        
        # ä½¿ç”¨åœºæ™¯æ¨è
        print("\nğŸ’¡ ä½¿ç”¨åœºæ™¯æ¨è:")
        print("-" * 50)
        
        # åŸºäºå®é™…æµ‹è¯•æ•°æ®ç»™å‡ºå»ºè®®
        batch_speedup = batch['ååé‡(req/s)']['vllm'] / batch['ååé‡(req/s)']['transformers']
        single_speedup = single['æ¨ç†æ—¶é—´(ç§’)']['transformers'] / single['æ¨ç†æ—¶é—´(ç§’)']['vllm']
        
        print("âœ… æ¨èä½¿ç”¨ vLLM çš„åœºæ™¯:")
        print(f"   â€¢ ç”Ÿäº§ç¯å¢ƒ API æœåŠ¡ (æ‰¹å¤„ç†å¿« {batch_speedup:.1f}x)")
        print(f"   â€¢ éœ€è¦é«˜ååé‡çš„åº”ç”¨ ({batch['ååé‡(req/s)']['vllm']:.1f} req/s vs {batch['ååé‡(req/s)']['transformers']:.1f} req/s)")
        print(f"   â€¢ å®æ—¶å¯¹è¯åº”ç”¨ (å•æ¬¡æ¨ç†å¿« {single_speedup:.1f}x)")
        
        print("\nâœ… æ¨èä½¿ç”¨ Transformers çš„åœºæ™¯:")
        if loading['åŠ è½½æ—¶é—´(ç§’)']['transformers'] < loading['åŠ è½½æ—¶é—´(ç§’)']['vllm']:
            print(f"   â€¢ å¿«é€ŸåŸå‹å¼€å‘ (åŠ è½½å¿« {loading['åŠ è½½æ—¶é—´(ç§’)']['vllm']/loading['åŠ è½½æ—¶é—´(ç§’)']['transformers']:.1f}x)")
        print("   â€¢ å­¦ä¹ å’Œç ”ç©¶ (ç”Ÿæ€ç³»ç»Ÿæˆç†Ÿ)")
        print("   â€¢ éœ€è¦ä¸°å¯Œé¢„å¤„ç†åŠŸèƒ½çš„åœºæ™¯")
        print("   â€¢ å¯¹éƒ¨ç½²å¤æ‚åº¦æ•æ„Ÿçš„é¡¹ç›®")
        
        # ä¿å­˜å®Œæ•´ç»“æœï¼ˆåŒ…å«å¯¹æ¯”åˆ†æï¼‰
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"intuitive_comparison_{timestamp}.json"
        
        # æ·»åŠ å¯¹æ¯”åˆ†æåˆ°ç»“æœä¸­
        self.results['performance_comparison'] = {
            'summary_table': summary_data,
            'key_findings': {
                'loading_speed': {
                    'winner': 'Transformers' if loading['åŠ è½½æ—¶é—´(ç§’)']['transformers'] < loading['åŠ è½½æ—¶é—´(ç§’)']['vllm'] else 'vLLM',
                    'advantage': f"{loading['åŠ è½½æ—¶é—´(ç§’)']['vllm']/loading['åŠ è½½æ—¶é—´(ç§’)']['transformers']:.1f}x" if loading['åŠ è½½æ—¶é—´(ç§’)']['transformers'] < loading['åŠ è½½æ—¶é—´(ç§’)']['vllm'] else f"{loading['åŠ è½½æ—¶é—´(ç§’)']['transformers']/loading['åŠ è½½æ—¶é—´(ç§’)']['vllm']:.1f}x",
                    'transformers_time': f"{loading['åŠ è½½æ—¶é—´(ç§’)']['transformers']:.1f}s",
                    'vllm_time': f"{loading['åŠ è½½æ—¶é—´(ç§’)']['vllm']:.1f}s"
                },
                'single_inference': {
                    'winner': 'Transformers' if single['æ¨ç†æ—¶é—´(ç§’)']['transformers'] < single['æ¨ç†æ—¶é—´(ç§’)']['vllm'] else 'vLLM',
                    'advantage': f"{single_speedup:.1f}x",
                    'transformers_time': f"{single['æ¨ç†æ—¶é—´(ç§’)']['transformers']:.2f}s",
                    'vllm_time': f"{single['æ¨ç†æ—¶é—´(ç§’)']['vllm']:.2f}s"
                },
                'batch_processing': {
                    'winner': 'vLLM' if batch_speedup > 1 else 'Transformers',
                    'advantage': f"{batch_speedup:.1f}x",
                    'transformers_throughput': f"{batch['ååé‡(req/s)']['transformers']:.1f} req/s",
                    'vllm_throughput': f"{batch['ååé‡(req/s)']['vllm']:.1f} req/s"
                }
            },
            'recommendations': {
                'use_vllm_for': [
                    f"ç”Ÿäº§ç¯å¢ƒ API æœåŠ¡ (æ‰¹å¤„ç†å¿« {batch_speedup:.1f}x)",
                    f"éœ€è¦é«˜ååé‡çš„åº”ç”¨ ({batch['ååé‡(req/s)']['vllm']:.1f} req/s vs {batch['ååé‡(req/s)']['transformers']:.1f} req/s)",
                    f"å®æ—¶å¯¹è¯åº”ç”¨ (å•æ¬¡æ¨ç†å¿« {single_speedup:.1f}x)"
                ],
                'use_transformers_for': [
                    f"å¿«é€ŸåŸå‹å¼€å‘ (åŠ è½½å¿« {loading['åŠ è½½æ—¶é—´(ç§’)']['vllm']/loading['åŠ è½½æ—¶é—´(ç§’)']['transformers']:.1f}x)" if loading['åŠ è½½æ—¶é—´(ç§’)']['transformers'] < loading['åŠ è½½æ—¶é—´(ç§’)']['vllm'] else "å¿«é€ŸåŸå‹å¼€å‘",
                    "å­¦ä¹ å’Œç ”ç©¶ (ç”Ÿæ€ç³»ç»Ÿæˆç†Ÿ)",
                    "éœ€è¦ä¸°å¯Œé¢„å¤„ç†åŠŸèƒ½çš„åœºæ™¯",
                    "å¯¹éƒ¨ç½²å¤æ‚åº¦æ•æ„Ÿçš„é¡¹ç›®"
                ]
            },
            'final_conclusion': {
                'overall_winner': 'vLLM' if batch_speedup > 2 or single_speedup > 1.5 else 'depends_on_use_case',
                'main_advantage': f"vLLM åœ¨æ‰¹å¤„ç†åœºæ™¯ä¸‹æœ‰æ˜¾è‘—ä¼˜åŠ¿ ({batch_speedup:.1f}x)" if batch_speedup > 2 else f"vLLM åœ¨å•æ¬¡æ¨ç†ä¸Šæœ‰æ˜æ˜¾ä¼˜åŠ¿ ({single_speedup:.1f}x)" if single_speedup > 1.5 else "ä¸¤ç§æ–¹æ¡ˆå„æœ‰ä¼˜åŠ¿ï¼Œæ ¹æ®å…·ä½“éœ€æ±‚é€‰æ‹©",
                'recommendation': "æ¨èç”¨äºç”Ÿäº§ç¯å¢ƒå’Œé«˜å¹¶å‘åœºæ™¯" if batch_speedup > 2 else "æ¨èç”¨äºå»¶è¿Ÿæ•æ„Ÿçš„åº”ç”¨" if single_speedup > 1.5 else "æ ¹æ®å…·ä½“éœ€æ±‚é€‰æ‹©"
            }
        }
        
        # æ·»åŠ æ‰¹å¤„ç†ä¼˜åŠ¿æ±‡æ€»
        batch_advantage_summary = {}
        for batch_size in [1, 5, 10, 20]:
            batch_key = f'batch_{batch_size}'
            if batch_key in results['batch_processing']:
                t_throughput = results['batch_processing'][batch_key]['ååé‡(req/s)']['transformers']
                v_throughput = results['batch_processing'][batch_key]['ååé‡(req/s)']['vllm']
                advantage = v_throughput / t_throughput
                batch_advantage_summary[f'batch_size_{batch_size}'] = {
                    'transformers_throughput': f"{t_throughput:.1f} req/s",
                    'vllm_throughput': f"{v_throughput:.1f} req/s",
                    'vllm_advantage': f"{advantage:.1f}x"
                }
        
        self.results['batch_advantage_summary'] = batch_advantage_summary
        
        # æ·»åŠ åœºæ™¯æµ‹è¯•æ€»ç»“
        scenario_summary = {}
        for scenario_name, scenario_data in results['real_world_scenarios'].items():
            t_time = scenario_data['åœºæ™¯æ€»æ—¶é—´(ç§’)']['transformers']
            v_time = scenario_data['åœºæ™¯æ€»æ—¶é—´(ç§’)']['vllm']
            speedup = t_time / v_time
            scenario_summary[scenario_name] = {
                'description': {
                    'èŠå¤©æœºå™¨äºº': 'æ¨¡æ‹ŸèŠå¤©æœºå™¨äººåœºæ™¯ - è¿ç»­5è½®å¯¹è¯',
                    'å†…å®¹ç”Ÿæˆ': 'æ¨¡æ‹Ÿå†…å®¹ç”Ÿæˆåœºæ™¯ - æ‰¹é‡åˆ›ä½œ',
                    'ä»£ç åŠ©æ‰‹': 'æ¨¡æ‹Ÿä»£ç åŠ©æ‰‹åœºæ™¯ - ç¼–ç¨‹é—®é¢˜è§£ç­”'
                }.get(scenario_name, ''),
                'transformers_time': f"{t_time:.2f}s",
                'vllm_time': f"{v_time:.2f}s",
                'vllm_advantage': f"{speedup:.1f}x",
                'winner': 'vLLM' if speedup > 1 else 'Transformers'
            }
        
        self.results['scenario_test_summary'] = scenario_summary
        
        # ä¿å­˜å¢å¼ºçš„ç»“æœ
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(self.results, f, indent=2, ensure_ascii=False)
        
        # åŒæ—¶ç”Ÿæˆä¸€ä¸ªç®€åŒ–çš„æ‘˜è¦æ–‡ä»¶
        summary_filename = f"comparison_summary_{timestamp}.json"
        summary_data_for_file = {
            'experiment_info': {
                'timestamp': self.results['timestamp'],
                'hardware': self.results['hardware'],
                'model': self.model_name,
                'purpose': 'Comprehensive vLLM vs Transformers comparison'
            },
            'key_findings': self.results['performance_comparison']['key_findings'],
            'batch_advantage': batch_advantage_summary,
            'scenario_results': scenario_summary,
            'recommendations': self.results['performance_comparison']['recommendations'],
            'final_conclusion': self.results['performance_comparison']['final_conclusion']
        }
        
        with open(summary_filename, 'w', encoding='utf-8') as f:
            json.dump(summary_data_for_file, f, indent=2, ensure_ascii=False)
        
        print(f"\nğŸ’¾ è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°: {filename}")
        print(f"ğŸ“‹ æ‘˜è¦æŠ¥å‘Šå·²ä¿å­˜åˆ°: {summary_filename}")
        
        # æœ€ç»ˆç»“è®º
        print(f"\nğŸ† æœ€ç»ˆç»“è®º:")
        print("-" * 30)
        if batch_speedup > 2:
            print(f"vLLM åœ¨æ‰¹å¤„ç†åœºæ™¯ä¸‹æœ‰æ˜¾è‘—ä¼˜åŠ¿ ({batch_speedup:.1f}x)")
            print("æ¨èç”¨äºç”Ÿäº§ç¯å¢ƒå’Œé«˜å¹¶å‘åœºæ™¯")
        elif single_speedup > 1.5:
            print(f"vLLM åœ¨å•æ¬¡æ¨ç†ä¸Šæœ‰æ˜æ˜¾ä¼˜åŠ¿ ({single_speedup:.1f}x)")
            print("æ¨èç”¨äºå»¶è¿Ÿæ•æ„Ÿçš„åº”ç”¨")
        else:
            print("ä¸¤ç§æ–¹æ¡ˆå„æœ‰ä¼˜åŠ¿ï¼Œæ ¹æ®å…·ä½“éœ€æ±‚é€‰æ‹©")
    
    def run_comprehensive_test(self):
        """è¿è¡Œå®Œæ•´çš„ç›´è§‚å¯¹æ¯”æµ‹è¯•"""
        print("ğŸš€ vLLM vs Transformers ç›´è§‚æ€§èƒ½å¯¹æ¯”å®éªŒ")
        print("=" * 80)
        print("ğŸ¯ å®éªŒç›®æ ‡ï¼šé€šè¿‡å…·ä½“æ•°æ®ç›´è§‚å±•ç¤ºä¸¤ç§æ¨ç†æ¡†æ¶çš„æ€§èƒ½å·®å¼‚")
        print("ğŸ“Š å®éªŒç‰¹ç‚¹ï¼šè¯¦ç»†æ•°æ®è¡¨æ ¼ + çœŸå®ä½¿ç”¨åœºæ™¯ + å®Œæ•´å®éªŒæ¡ä»¶")
        print("â±ï¸ é¢„è®¡æ—¶é—´ï¼š10-15åˆ†é’Ÿ")
        print()
        
        # é¦–å…ˆè¯¦ç»†è¾“å‡ºå®éªŒæ¡ä»¶
        self.print_experiment_conditions()
        
        try:
            # ä¾æ¬¡æ‰§è¡Œå„é¡¹æµ‹è¯•
            self.test_loading_performance()
            self.test_single_inference()
            self.test_batch_processing()
            self.test_real_world_scenarios()
            self.generate_final_summary()
            
            print("\nğŸ‰ å¯¹æ¯”æµ‹è¯•å®Œæˆï¼")
            
        except Exception as e:
            print(f"âŒ æµ‹è¯•å¤±è´¥: {str(e)}")
            import traceback
            traceback.print_exc()
        finally:
            # æ¸…ç†èµ„æº
            if hasattr(self, 'transformers_pipe'):
                del self.transformers_pipe
            if hasattr(self, 'vllm_llm'):
                del self.vllm_llm
            clear_cache()

def main():
    """ä¸»å‡½æ•°"""
    comparison = IntuitiveComparison()
    comparison.run_comprehensive_test()

if __name__ == "__main__":
    main()
